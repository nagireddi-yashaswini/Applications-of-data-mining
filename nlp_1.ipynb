{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nagireddi-yashaswini/Applications-of-data-mining/blob/main/nlp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD99ZBFNJ3Jd",
        "outputId": "0a8c044d-0fb5-46db-e0c3-889b003aa4ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== First 3 rows ===\n",
            "                                         resume_text\n",
            "0  John Doe\\nSoftware Engineer • Skilled in Pytho...\n",
            "1  Jane Smith - Data Analyst • Proficient in R, P...\n",
            "2  Michael Johnson\\nProject Manager • Expertise i... \n",
            "\n",
            "=== Noisy characters in first resume ===\n",
            "{'x', 'S', 'P', '•', 'i', 'k', 'h', 'p', 'o', 'z', ' ', '\\n', 'A', 'Q', 'w', 'd', 'l', 'L', 'v', 'W', 'y', 'g', 'E', 't', ')', 'J', 'n', 'D', ',', 'u', 'a', 'f', '.', 'r', '(', 'e', 'c'} \n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== Top 10 frequent stemmed words (NLTK) ===\n",
            "[('python', 2), ('john', 1), ('doe', 1), ('softwar', 1), ('engin', 1), ('skill', 1), ('java', 1), ('sql', 1), ('experi', 1), ('cloud', 1)] \n",
            "\n",
            "=== Top 10 frequent lemmas (spaCy) ===\n",
            "[('python', 2), ('software', 1), ('engineer', 1), ('experience', 1), ('cloud', 1), ('aw', 1), ('azure', 1), ('analyst', 1), ('proficient', 1), ('r', 1)] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "\n",
        "# -----------------------------\n",
        "# Section A: Load & Explore\n",
        "# -----------------------------\n",
        "data = {\n",
        "    \"resume_text\": [\n",
        "        \"John Doe\\nSoftware Engineer • Skilled in Python, Java, SQL. Experience in cloud (AWS, Azure).\",\n",
        "        \"Jane Smith - Data Analyst • Proficient in R, Python, Tableau, and machine learning techniques.\",\n",
        "        \"Michael Johnson\\nProject Manager • Expertise in Agile, Scrum, Leadership & Communication.\"\n",
        "    ]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"=== First 3 rows ===\")\n",
        "print(df.head(3), \"\\n\")\n",
        "\n",
        "print(\"=== Noisy characters in first resume ===\")\n",
        "print(set(\"\".join(df['resume_text'][0])), \"\\n\")\n",
        "\n",
        "\n",
        "# Section B: NLTK Preprocessing\n",
        "\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_and_tokenize(text):\n",
        "    text = re.sub(r\"[^a-zA-Z\\s]\", \" \", text)  # remove digits & symbols\n",
        "    text = text.lower()\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stemmer.stem(word) for word in tokens if word not in stop_words]\n",
        "    return tokens\n",
        "\n",
        "df[\"tokens_nltk\"] = df[\"resume_text\"].apply(clean_and_tokenize)\n",
        "\n",
        "# Top 10 frequent stemmed words\n",
        "all_tokens = [word for tokens in df[\"tokens_nltk\"] for word in tokens]\n",
        "freq_words_nltk = Counter(all_tokens).most_common(10)\n",
        "\n",
        "print(\"=== Top 10 frequent stemmed words (NLTK) ===\")\n",
        "print(freq_words_nltk, \"\\n\")\n",
        "\n",
        "\n",
        "# Section C: spaCy Pipeline\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def spacy_preprocess(text):\n",
        "    doc = nlp(text.lower())\n",
        "    lemmas = [\n",
        "        token.lemma_ for token in doc\n",
        "        if token.is_alpha and token.pos_ in [\"NOUN\", \"VERB\"]\n",
        "    ]\n",
        "    return lemmas\n",
        "\n",
        "df[\"tokens_spacy\"] = df[\"resume_text\"].apply(spacy_preprocess)\n",
        "\n",
        "# Top 10 frequent lemmas\n",
        "all_lemmas = [lemma for lemmas in df[\"tokens_spacy\"] for lemma in lemmas]\n",
        "freq_words_spacy = Counter(all_lemmas).most_common(10)\n",
        "\n",
        "print(\"=== Top 10 frequent lemmas (spaCy) ===\")\n",
        "print(freq_words_spacy, \"\\n\")"
      ]
    }
  ]
}